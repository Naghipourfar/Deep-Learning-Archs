{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST_CNN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Naghipourfar/Deep-Learning-Course/blob/master/Code/MNIST_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "op_BJmZYXUNH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import scipy\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.ops import array_ops\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s-3NyzPGXaXZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def max_pooling(x, pooling_shape=(2, 2), name_scope=\"MaxPooling_1\"):\n",
        "    with tf.name_scope(name_scope):\n",
        "        kernel_size = [1, pooling_shape[0], pooling_shape[1], 1]\n",
        "        strides = [1, pooling_shape[0], pooling_shape[1], 1]\n",
        "        output = tf.nn.max_pool(x, kernel_size, strides, padding=\"VALID\")\n",
        "        return output\n",
        "\n",
        "\n",
        "def batch_normalization_from_scratch_conv(x, n_out, training_phase, name_scope=\"BatchNormalization\"):\n",
        "    epsilon = 1e-7\n",
        "    with tf.name_scope(name_scope):\n",
        "        # n_out is number of filters for conv\n",
        "        gamma = tf.Variable(tf.constant(1.0, shape=[n_out]), trainable=True, name=\"Gamma\")\n",
        "        beta = tf.Variable(tf.constant(0.0, shape=[n_out]), trainable=True, name=\"Beta\")\n",
        "\n",
        "        ema = tf.train.ExponentialMovingAverage(decay=0.5)\n",
        "\n",
        "        def calculate_moments_for_conv():  # calculate mean, var for conv layer\n",
        "            batch_mean = tf.reduce_mean(x, axis=[0, 1, 2], keep_dims=True)\n",
        "            batch_mean = array_ops.squeeze(batch_mean, [0, 1, 2])\n",
        "            m = tf.reduce_mean(x, axis=[0, 1, 2], keep_dims=True)\n",
        "            squared_diffs = tf.square(x - m)\n",
        "            batch_var = tf.reduce_mean(squared_diffs, axis=[0, 1, 2], keep_dims=True)\n",
        "            batch_var = array_ops.squeeze(batch_var, [0, 1, 2])\n",
        "            return batch_mean, batch_var\n",
        "\n",
        "        batch_mean, batch_var = calculate_moments_for_conv()\n",
        "\n",
        "        def mean_var_with_update_at_train_time():  # Track & save the mean and variance for test time\n",
        "            ema_apply_op = ema.apply([batch_mean, batch_var])\n",
        "            with tf.control_dependencies([ema_apply_op]):\n",
        "                return tf.identity(batch_mean), tf.identity(batch_var)\n",
        "\n",
        "        mean, var = tf.cond(training_phase,  # calculate mean, var for train/test time\n",
        "                            mean_var_with_update_at_train_time,\n",
        "                            lambda: (ema.average(batch_mean), ema.average(batch_var)))\n",
        "\n",
        "        denominator = tf.rsqrt(var + epsilon)  # NOTE: rsqrt -> 1 / sqrt()\n",
        "        normed = gamma * (x - mean) * denominator + beta\n",
        "    return normed\n",
        "\n",
        "\n",
        "def batch_normalization_from_scratch_dense(x, n_out, training_phase, name_scope=\"BatchNormalization\"):\n",
        "    epsilon = 1e-7\n",
        "    with tf.name_scope(name_scope):\n",
        "        # n_out is number of nodes in fully connected layer\n",
        "        gamma = tf.Variable(tf.constant(1.0, shape=[n_out]), trainable=True, name=\"Gamma\")\n",
        "        beta = tf.Variable(tf.constant(0.0, shape=[n_out]), trainable=True, name=\"Beta\")\n",
        "\n",
        "        ema = tf.train.ExponentialMovingAverage(decay=0.5)\n",
        "\n",
        "        def calculate_moments_for_dense():  # calculate mean, var for fully connected layer\n",
        "            batch_mean = tf.reduce_mean(x, axis=[0], keep_dims=True)\n",
        "            m = tf.reduce_mean(x, axis=[0], keep_dims=True)\n",
        "            devs_squared = tf.square(x - m)\n",
        "            batch_var = tf.reduce_mean(devs_squared, axis=[0], keep_dims=True)\n",
        "            return batch_mean, batch_var\n",
        "\n",
        "        batch_mean, batch_var = calculate_moments_for_dense()\n",
        "\n",
        "        def mean_var_with_update_at_train_time():  # Track & save the mean and variance for test time\n",
        "            ema_apply_op = ema.apply([batch_mean, batch_var])\n",
        "            with tf.control_dependencies([ema_apply_op]):\n",
        "                return tf.identity(batch_mean), tf.identity(batch_var)\n",
        "\n",
        "        mean, var = tf.cond(training_phase,  # calculate mean, var for train/test time\n",
        "                            mean_var_with_update_at_train_time,\n",
        "                            lambda: (ema.average(batch_mean), ema.average(batch_var)))\n",
        "\n",
        "        denominator = tf.rsqrt(var + epsilon)  # NOTE: rsqrt -> 1 / sqrt()\n",
        "        normed = gamma * (x - mean) * denominator + beta\n",
        "    return normed\n",
        "  \n",
        "def dropout(x, keep_prob, training_phase):\n",
        "    with tf.name_scope(\"Dropout\"):\n",
        "        input_shape = x.get_shape().as_list()\n",
        "        m = tf.cond(training_phase,\n",
        "                    lambda: np.random.binomial(1, keep_prob, size=input_shape),\n",
        "                    lambda: tf.constant(keep_prob, shape=input_shape))\n",
        "        m = tf.multiply(x, m)\n",
        "        return m\n",
        "def batch_normalization(x, n_out, phase_train):\n",
        "    with tf.variable_scope('BatchNormalization'):\n",
        "        beta = tf.Variable(tf.constant(0.0, shape=[n_out]),\n",
        "                           name='beta', trainable=True)\n",
        "        gamma = tf.Variable(tf.constant(1.0, shape=[n_out]),\n",
        "                            name='gamma', trainable=True)\n",
        "        batch_mean, batch_var = tf.nn.moments(x, [0, 1, 2], name='moments')\n",
        "        ema = tf.train.ExponentialMovingAverage(decay=0.5)\n",
        "\n",
        "        def mean_var_with_update():\n",
        "            ema_apply_op = ema.apply([batch_mean, batch_var])\n",
        "            with tf.control_dependencies([ema_apply_op]):\n",
        "                return tf.identity(batch_mean), tf.identity(batch_var)\n",
        "\n",
        "        mean, var = tf.cond(phase_train,\n",
        "                            mean_var_with_update,\n",
        "                            lambda: (ema.average(batch_mean), ema.average(batch_var)))\n",
        "        normed = tf.nn.batch_normalization(x, mean, var, beta, gamma, 1e-3)\n",
        "    return normed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q2QVT919Xib_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class CNN(object):\n",
        "    def __init__(self, filters=None, filter_windows=None, pooling_shapes=None, n_outputs=10,\n",
        "                 save_folder=None, task_num=1):\n",
        "        tf.reset_default_graph()\n",
        "        if filters is None:\n",
        "            filters = [64, 64]\n",
        "        if filter_windows is None:\n",
        "            filter_windows = [(5, 5), (5, 5)]\n",
        "        if pooling_shapes is None:\n",
        "            pooling_shapes = [(2, 2), (2, 2)]\n",
        "        self.filters = filters\n",
        "        self.filter_windows = filter_windows\n",
        "        self.pooling_shapes = pooling_shapes\n",
        "        self.x = tf.placeholder(dtype=tf.float32, shape=(None, 784), name=\"input\")\n",
        "        self.y = tf.placeholder(dtype=tf.float32, shape=(None, n_outputs), name=\"label\")\n",
        "        self.keep_prob = tf.placeholder(tf.float32, name=\"Dropout_rate\")\n",
        "        self.phase_train = tf.placeholder(tf.bool, name='training_phase')\n",
        "        self.convolution_weights = {}\n",
        "        self.convolution_layers = {}\n",
        "        self.task_num = task_num\n",
        "        if task_num == 1:\n",
        "            output = self.create_CNN_without_regularization()\n",
        "        elif task_num == 2:\n",
        "            output = self.create_CNN_with_BN()\n",
        "        elif task_num == 3:\n",
        "            output = self.create_CNN_with_dropout()\n",
        "        else:\n",
        "            output = self.create_CNN_with_BN_and_dropout()\n",
        "        self.variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"input\")\n",
        "        self.variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"Conv_1\")\n",
        "        self.variables += tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"MaxPooling_1\")\n",
        "        self.variables += tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"Conv_2\")\n",
        "        self.variables += tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"MaxPooling_2\")\n",
        "        self.variables += tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"Dense_1\")\n",
        "        self.compile(output)\n",
        "        self.save_folder = save_folder\n",
        "        if os.path.exists(\"../Results/tensorboard/\" + save_folder + \"_train\"):\n",
        "            import shutil\n",
        "            shutil.rmtree(\"../Results/tensorboard/\" + save_folder + \"_train\")\n",
        "            shutil.rmtree(\"../Results/tensorboard/\" + save_folder + \"_test\")\n",
        "        os.makedirs(\"../Results/tensorboard/\" + save_folder + \"_train\", exist_ok=True)\n",
        "        os.makedirs(\"../Results/tensorboard/\" + save_folder + \"_test\", exist_ok=True)\n",
        "\n",
        "    def create_CNN_without_regularization(self):\n",
        "        max_pool = tf.reshape(self.x, shape=(-1, 28, 28, 1), name=\"input\")\n",
        "        for i in range(len(self.filters)):\n",
        "            convolution = self.convolution_layer(max_pool,\n",
        "                                                 filters=self.filters[i],\n",
        "                                                 filter_window=self.filter_windows[i],\n",
        "                                                 stride=1,\n",
        "                                                 n_input_channels=max_pool.get_shape().as_list()[3],\n",
        "                                                 name_scope=\"Conv_%d\" % (i + 1))\n",
        "            convolution = tf.nn.relu(convolution)\n",
        "            self.convolution_layers[\"Conv_%d\" % (i + 1)] = convolution\n",
        "            max_pool = max_pooling(convolution,\n",
        "                                   pooling_shape=self.pooling_shapes[i],\n",
        "                                   name_scope=\"MaxPooling_%d\" % (i + 1))\n",
        "        n_flatten_neurons = 1\n",
        "        for i in max_pool.get_shape().as_list()[1:]:\n",
        "            n_flatten_neurons *= i\n",
        "        flatten = tf.reshape(max_pool, [-1, n_flatten_neurons])\n",
        "        fully_connected_layer = self.dense(flatten, flatten.get_shape().as_list()[1], 256, name_scope=\"Dense_1\")\n",
        "        fully_connected_layer = tf.nn.relu(fully_connected_layer)\n",
        "        output_layer = self.dense(fully_connected_layer, 256, 10, name_scope=\"Output_Layer\")\n",
        "        output_layer = tf.nn.softmax(output_layer)\n",
        "        return output_layer\n",
        "\n",
        "    def create_CNN_with_BN(self):\n",
        "        max_pool = tf.reshape(self.x, shape=(-1, 28, 28, 1), name=\"input\")\n",
        "        for i in range(len(self.filters)):\n",
        "            convolution = self.convolution_layer(max_pool,\n",
        "                                                 filters=self.filters[i],\n",
        "                                                 filter_window=self.filter_windows[i],\n",
        "                                                 stride=1,\n",
        "                                                 n_input_channels=max_pool.get_shape().as_list()[3],\n",
        "                                                 name_scope=\"Conv_%d\" % (i + 1), trainable=False)\n",
        "            convolution = batch_normalization_from_scratch_conv(convolution, self.filters[i], self.phase_train)\n",
        "            convolution = tf.nn.relu(convolution)\n",
        "            self.convolution_layers[\"Conv_%d\" % (i + 1)] = convolution\n",
        "            max_pool = max_pooling(convolution,\n",
        "                                   pooling_shape=self.pooling_shapes[i],\n",
        "                                   name_scope=\"MaxPooling_%d\" % (i + 1))\n",
        "        n_flatten_neurons = 1\n",
        "        for i in max_pool.get_shape().as_list()[1:]:\n",
        "            n_flatten_neurons *= i\n",
        "        flatten = tf.reshape(max_pool, [-1, n_flatten_neurons])\n",
        "\n",
        "        fully_connected_layer = self.dense(flatten, flatten.get_shape().as_list()[1], 256, name_scope=\"Dense_1\",\n",
        "                                           trainable=False)\n",
        "        fully_connected_layer = batch_normalization_from_scratch_dense(fully_connected_layer, 256, self.phase_train)\n",
        "        fully_connected_layer = tf.nn.relu(fully_connected_layer)\n",
        "        output_layer = self.dense(fully_connected_layer, 256, 10, name_scope=\"Output_Layer\")\n",
        "        output_layer = tf.nn.softmax(output_layer)\n",
        "        return output_layer\n",
        "\n",
        "    def create_CNN_with_dropout(self):\n",
        "        max_pool = tf.reshape(self.x, shape=(-1, 28, 28, 1), name=\"input\")\n",
        "        for i in range(len(self.filters)):\n",
        "            convolution = self.convolution_layer(max_pool,\n",
        "                                                 filters=self.filters[i],\n",
        "                                                 filter_window=self.filter_windows[i],\n",
        "                                                 stride=1,\n",
        "                                                 n_input_channels=max_pool.get_shape().as_list()[3],\n",
        "                                                 name_scope=\"Conv_%d\" % (i + 1), trainable=False)\n",
        "            convolution = tf.nn.relu(convolution)\n",
        "            convolution = dropout(convolution, 0.25, self.phase_train)\n",
        "            self.convolution_layers[\"Conv_%d\" % (i + 1)] = convolution\n",
        "            max_pool = max_pooling(convolution,\n",
        "                                   pooling_shape=self.pooling_shapes[i],\n",
        "                                   name_scope=\"MaxPooling_%d\" % (i + 1))\n",
        "        n_flatten_neurons = 1\n",
        "        for i in max_pool.get_shape().as_list()[1:]:\n",
        "            n_flatten_neurons *= i\n",
        "        flatten = tf.reshape(max_pool, [-1, n_flatten_neurons])\n",
        "\n",
        "        fully_connected_layer = self.dense(flatten, flatten.get_shape().as_list()[1], 256, name_scope=\"Dense_1\",\n",
        "                                           trainable=False)\n",
        "        fully_connected_layer = tf.nn.relu(fully_connected_layer)\n",
        "        fully_connected_layer = dropout(fully_connected_layer, 0.25, self.phase_train)\n",
        "        output_layer = self.dense(fully_connected_layer, 256, 10, name_scope=\"Output_Layer\")\n",
        "        output_layer = tf.nn.softmax(output_layer)\n",
        "        return output_layer\n",
        "\n",
        "    def create_CNN_with_BN_and_dropout(self):\n",
        "        max_pool = tf.reshape(self.x, shape=(-1, 28, 28, 1), name=\"input\")\n",
        "        for i in range(len(self.filters)):\n",
        "            convolution = self.convolution_layer(max_pool,\n",
        "                                                 filters=self.filters[i],\n",
        "                                                 filter_window=self.filter_windows[i],\n",
        "                                                 stride=1,\n",
        "                                                 n_input_channels=max_pool.get_shape().as_list()[3],\n",
        "                                                 name_scope=\"Conv_%d\" % (i + 1), trainable=False)\n",
        "            convolution = batch_normalization_from_scratch_conv(convolution, self.filters[i], self.phase_train)\n",
        "            convolution = tf.nn.relu(convolution)\n",
        "            convolution = dropout(convolution, 0.25, self.phase_train)\n",
        "            self.convolution_layers[\"Conv_%d\" % (i + 1)] = convolution\n",
        "            max_pool = max_pooling(convolution,\n",
        "                                   pooling_shape=self.pooling_shapes[i],\n",
        "                                   name_scope=\"MaxPooling_%d\" % (i + 1))\n",
        "        n_flatten_neurons = 1\n",
        "        for i in max_pool.get_shape().as_list()[1:]:\n",
        "            n_flatten_neurons *= i\n",
        "        flatten = tf.reshape(max_pool, [-1, n_flatten_neurons])\n",
        "\n",
        "        fully_connected_layer = self.dense(flatten, flatten.get_shape().as_list()[1], 256, name_scope=\"Dense_1\",\n",
        "                                           trainable=False)\n",
        "        fully_connected_layer = batch_normalization_from_scratch_dense(fully_connected_layer, 256, self.phase_train)\n",
        "        fully_connected_layer = tf.nn.relu(fully_connected_layer)\n",
        "        fully_connected_layer = dropout(fully_connected_layer, 0.25, self.phase_train)\n",
        "        output_layer = self.dense(fully_connected_layer, 256, 10, name_scope=\"Output_Layer\")\n",
        "        output_layer = tf.nn.softmax(output_layer)\n",
        "        return output_layer\n",
        "\n",
        "    def convolution_layer(self, x, filters=64, filter_window=(5, 5), stride=1, n_input_channels=1,\n",
        "                          name_scope=\"Conv2D_1\", trainable=True):\n",
        "        with tf.name_scope(name_scope):\n",
        "            weights = tf.Variable(\n",
        "                tf.truncated_normal(shape=[filter_window[0], filter_window[1], n_input_channels, filters], mean=0.0,\n",
        "                                    stddev=0.1),\n",
        "                name=\"Weights\", trainable=trainable)\n",
        "            self.convolution_weights[name_scope] = weights\n",
        "            biases = tf.Variable(tf.truncated_normal(shape=[filters], mean=0.0, stddev=0.1), name=\"Bias\",\n",
        "                                 trainable=trainable)\n",
        "            convolution = tf.nn.conv2d(x, weights, strides=[1, stride, stride, 1], padding=\"VALID\")\n",
        "\n",
        "            convolution += biases\n",
        "\n",
        "            # tf.summary.image(\"Weights\", weights)\n",
        "            # tf.summary.image(\"Biases\", biases)\n",
        "            return convolution\n",
        "\n",
        "    def compile(self, output):\n",
        "        with tf.name_scope(\"Cross_Entropy\"):\n",
        "            self.cross_entropy = tf.reduce_mean(\n",
        "                tf.nn.softmax_cross_entropy_with_logits_v2(labels=self.y,\n",
        "                                                        logits=output))\n",
        "\n",
        "        with tf.name_scope(\"Train\"):\n",
        "            self.optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-2).minimize(self.cross_entropy)\n",
        "        with tf.name_scope(\"accuracy\"):\n",
        "            correct_prediction = tf.equal(tf.argmax(output, axis=1),\n",
        "                                          tf.argmax(self.y, axis=1))\n",
        "            self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, dtype=tf.float32), name=\"train_accuracy\")\n",
        "\n",
        "        tf.summary.scalar(\"Cross_Entropy\", self.cross_entropy)\n",
        "        tf.summary.scalar(\"accuracy\", self.accuracy)\n",
        "\n",
        "    def dense(self, x, n_input_neurons, n_output_neurons, name_scope=\"Dense_1\", trainable=True):\n",
        "        with tf.name_scope(name_scope):\n",
        "            w = tf.Variable(tf.truncated_normal(shape=(n_input_neurons, n_output_neurons),\n",
        "                                                mean=0, stddev=0.1, seed=2018),\n",
        "                            name=\"Weight\", trainable=trainable)\n",
        "            b = tf.Variable(tf.constant(0.1, shape=[n_output_neurons]), name=\"Bias\", trainable=trainable)\n",
        "            hidden_output = tf.add(tf.matmul(x, w), b)\n",
        "            tf.summary.histogram(\"Weights\", w)\n",
        "\n",
        "            return hidden_output\n",
        "\n",
        "    def fit(self, n_epochs=50000, batch_size=64, verbose=1, keep_prob=0.5):\n",
        "        merge = tf.summary.merge_all()\n",
        "        init = tf.global_variables_initializer()\n",
        "        saver = tf.train.Saver()\n",
        "        with tf.Session() as sess:\n",
        "            train_file_writer = tf.summary.FileWriter(\n",
        "                logdir=\"../Results/tensorboard/%s\" % self.save_folder + \"_train\",\n",
        "                graph=sess.graph)\n",
        "            test_file_writer = tf.summary.FileWriter(\n",
        "                logdir=\"../Results/tensorboard/%s\" % self.save_folder + \"_test\",\n",
        "                graph=sess.graph)\n",
        "\n",
        "            sess.run(init)\n",
        "\n",
        "            sess.run(tf.global_variables_initializer())\n",
        "            mnist = load_data()\n",
        "            test_batch_xs, test_batch_ys = mnist.validation.next_batch(10000)\n",
        "\n",
        "            for i in range(n_epochs):\n",
        "                train_batch_xs, train_batch_ys = mnist.train.next_batch(batch_size)\n",
        "                sess.run(self.optimizer,\n",
        "                         feed_dict={self.x: train_batch_xs, self.y: train_batch_ys, self.keep_prob: keep_prob,\n",
        "                                    self.phase_train: True})\n",
        "                if (i + 1) % 50 == 0:\n",
        "                    summaries = sess.run(merge, feed_dict={self.x: train_batch_xs, self.y: train_batch_ys,\n",
        "                                                           self.keep_prob: keep_prob, self.phase_train: True})\n",
        "                    train_file_writer.add_summary(summaries, i)\n",
        "\n",
        "                    summaries = sess.run(merge, feed_dict={self.x: test_batch_xs, self.y: test_batch_ys,\n",
        "                                                           self.keep_prob: keep_prob, self.phase_train: False})\n",
        "                    test_file_writer.add_summary(summaries, i)\n",
        "\n",
        "                    train_accuracy, train_loss = sess.run((self.accuracy, self.cross_entropy),\n",
        "                                                          feed_dict={self.x: train_batch_xs, self.y: train_batch_ys,\n",
        "                                                                     self.keep_prob: keep_prob, self.phase_train: True})\n",
        "\n",
        "                    test_accuracy, test_loss = sess.run([self.accuracy, self.cross_entropy],\n",
        "                                                        feed_dict={self.x: test_batch_xs, self.y: test_batch_ys,\n",
        "                                                                   self.keep_prob: keep_prob, self.phase_train: False})\n",
        "                    if verbose == 1:\n",
        "                        print(\n",
        "                            \"Epoch: %5i\\t Train Accuracy: %.4f %%\\t Train Loss: %.4f\\t Validation Accuracy: %.4f %%\\t \"\n",
        "                            \"Validation \"\n",
        "                            \"Loss: %.4f\" % (\n",
        "                                i + 1, 100.0 * train_accuracy, train_loss, 100.0 * test_accuracy, test_loss))\n",
        "                    chkpt_path = \"../Results/CNN_iter_%4d.ckpt\" % (i + 1)\n",
        "                    saver.save(sess, chkpt_path)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ecIx0-qZXk7w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_data():\n",
        "    from tensorflow.examples.tutorials.mnist import input_data\n",
        "    mnist = input_data.read_data_sets(\"mnist\", one_hot=True)\n",
        "    return mnist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X8Eg_UJ2XmHn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "filters = [64, 64]\n",
        "filter_windows = [(5, 5), (5, 5)]\n",
        "pooling_shapes = [(2, 2), (2, 2)]\n",
        "!rm -r ../Results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FMpL4SNFdGyK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = CNN(filters=filters, filter_windows=filter_windows, pooling_shapes=pooling_shapes, n_outputs=10, save_folder=\"CNN_without_Regularization\", task_num=1)\n",
        "model.fit(n_epochs=20000, batch_size=64, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TXS7tx-cdIv5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!rm -r ../Results/tensorboard/CNN_with_BN_train\n",
        "!rm -r ../Results/tensorboard/CNN_with_BN_test\n",
        "model = CNN(filters=filters, filter_windows=filter_windows, pooling_shapes=pooling_shapes, n_outputs=10, save_folder=\"CNN_with_BN\", task_num=2)\n",
        "model.fit(n_epochs=20000, batch_size=64, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H6PcdRQwdUVp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!rm -r ../Results/tensorboard/CNN_with_dropout_train\n",
        "!rm -r ../Results/tensorboard/CNN_with_dropout_test\n",
        "model = CNN(filters=filters, filter_windows=filter_windows, pooling_shapes=pooling_shapes, n_outputs=10, save_folder=\"CNN_with_dropout\", task_num=3)\n",
        "model.fit(n_epochs=20000, batch_size=64, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0oTU_t4IdW82",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!rm -r ../Results/tensorboard/CNN_with_Regularization_train\n",
        "!rm -r ../Results/tensorboard/CNN_with_Regularization_test\n",
        "model = CNN(filters=filters, filter_windows=filter_windows, pooling_shapes=pooling_shapes, n_outputs=10, save_folder=\"CNN_with_Regularization\", task_num=4)\n",
        "model.fit(n_epochs=20000, batch_size=64, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NL-flL9nYHV5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# !ls ../Results\n",
        "!tar -zcvf ../Results.tar.gz ../Results\n",
        "from google.colab import files\n",
        "files.download('../Results.tar.gz')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}