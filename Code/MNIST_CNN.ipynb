{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST_CNN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Naghipourfar/Deep-Learning-Course/blob/master/Code/MNIST_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "op_BJmZYXUNH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import scipy\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.ops import array_ops\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s-3NyzPGXaXZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def max_pooling(x, pooling_shape=(2, 2), name_scope=\"MaxPooling_1\"):\n",
        "    with tf.name_scope(name_scope):\n",
        "        kernel_size = [1, pooling_shape[0], pooling_shape[1], 1]\n",
        "        strides = [1, pooling_shape[0], pooling_shape[1], 1]\n",
        "        output = tf.nn.max_pool(x, kernel_size, strides, padding=\"VALID\")\n",
        "        return output\n",
        "\n",
        "\n",
        "def batch_normalization_from_scratch_conv(x, n_out, training_phase, name_scope=\"BatchNormalization\"):\n",
        "    epsilon = 1e-7\n",
        "    with tf.name_scope(name_scope):\n",
        "        # n_out is number of filters for conv\n",
        "        gamma = tf.Variable(tf.constant(1.0, shape=[n_out]), trainable=True, name=\"Gamma\")\n",
        "        beta = tf.Variable(tf.constant(0.0, shape=[n_out]), trainable=True, name=\"Beta\")\n",
        "\n",
        "        ema = tf.train.ExponentialMovingAverage(decay=0.5)\n",
        "\n",
        "        def calculate_moments_for_conv():  # calculate mean, var for conv layer\n",
        "            batch_mean = tf.reduce_mean(x, axis=[0, 1, 2], keep_dims=True)\n",
        "            batch_mean = array_ops.squeeze(batch_mean, [0, 1, 2])\n",
        "            m = tf.reduce_mean(x, axis=[0, 1, 2], keep_dims=True)\n",
        "            squared_diffs = tf.square(x - m)\n",
        "            batch_var = tf.reduce_mean(squared_diffs, axis=[0, 1, 2], keep_dims=True)\n",
        "            batch_var = array_ops.squeeze(batch_var, [0, 1, 2])\n",
        "            return batch_mean, batch_var\n",
        "\n",
        "        batch_mean, batch_var = calculate_moments_for_conv()\n",
        "\n",
        "        def mean_var_with_update_at_train_time():  # Track & save the mean and variance for test time\n",
        "            ema_apply_op = ema.apply([batch_mean, batch_var])\n",
        "            with tf.control_dependencies([ema_apply_op]):\n",
        "                return tf.identity(batch_mean), tf.identity(batch_var)\n",
        "\n",
        "        mean, var = tf.cond(training_phase,  # calculate mean, var for train/test time\n",
        "                            mean_var_with_update_at_train_time,\n",
        "                            lambda: (ema.average(batch_mean), ema.average(batch_var)))\n",
        "\n",
        "        denominator = tf.rsqrt(var + epsilon)  # NOTE: rsqrt -> 1 / sqrt()\n",
        "        normed = gamma * (x - mean) * denominator + beta\n",
        "    return normed\n",
        "\n",
        "\n",
        "def batch_normalization_from_scratch_dense(x, n_out, training_phase, name_scope=\"BatchNormalization\"):\n",
        "    epsilon = 1e-7\n",
        "    with tf.name_scope(name_scope):\n",
        "        # n_out is number of nodes in fully connected layer\n",
        "        gamma = tf.Variable(tf.constant(1.0, shape=[n_out]), trainable=True, name=\"Gamma\")\n",
        "        beta = tf.Variable(tf.constant(0.0, shape=[n_out]), trainable=True, name=\"Beta\")\n",
        "\n",
        "        ema = tf.train.ExponentialMovingAverage(decay=0.5)\n",
        "\n",
        "        def calculate_moments_for_dense():  # calculate mean, var for fully connected layer\n",
        "            batch_mean = tf.reduce_mean(x, axis=[0], keep_dims=True)\n",
        "            m = tf.reduce_mean(x, axis=[0], keep_dims=True)\n",
        "            devs_squared = tf.square(x - m)\n",
        "            batch_var = tf.reduce_mean(devs_squared, axis=[0], keep_dims=True)\n",
        "            return batch_mean, batch_var\n",
        "\n",
        "        batch_mean, batch_var = calculate_moments_for_dense()\n",
        "\n",
        "        def mean_var_with_update_at_train_time():  # Track & save the mean and variance for test time\n",
        "            ema_apply_op = ema.apply([batch_mean, batch_var])\n",
        "            with tf.control_dependencies([ema_apply_op]):\n",
        "                return tf.identity(batch_mean), tf.identity(batch_var)\n",
        "\n",
        "        mean, var = tf.cond(training_phase,  # calculate mean, var for train/test time\n",
        "                            mean_var_with_update_at_train_time,\n",
        "                            lambda: (ema.average(batch_mean), ema.average(batch_var)))\n",
        "\n",
        "        denominator = tf.rsqrt(var + epsilon)  # NOTE: rsqrt -> 1 / sqrt()\n",
        "        normed = gamma * (x - mean) * denominator + beta\n",
        "    return normed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q2QVT919Xib_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class CNN(object):\n",
        "    def __init__(self, filters=None, filter_windows=None, pooling_shapes=None, n_outputs=10,\n",
        "                 save_folder=None, task_num=1):\n",
        "        tf.reset_default_graph()\n",
        "        if filters is None:\n",
        "            filters = [64, 64]\n",
        "        if filter_windows is None:\n",
        "            filter_windows = [(5, 5), (5, 5)]\n",
        "        if pooling_shapes is None:\n",
        "            pooling_shapes = [(2, 2), (2, 2)]\n",
        "        self.filters = filters\n",
        "        self.filter_windows = filter_windows\n",
        "        self.pooling_shapes = pooling_shapes\n",
        "        self.x = tf.placeholder(dtype=tf.float32, shape=(None, 784), name=\"input\")\n",
        "        self.y = tf.placeholder(dtype=tf.float32, shape=(None, n_outputs), name=\"label\")\n",
        "        self.keep_prob = tf.placeholder(tf.float32, name=\"Dropout_rate\")\n",
        "        self.phase_train = tf.placeholder(tf.bool, name='training_phase')\n",
        "        self.convolution_weights = {}\n",
        "        self.convolution_layers = {}\n",
        "        self.task_num = task_num\n",
        "        if task_num == 1:\n",
        "            output = self.create_CNN()\n",
        "        elif task_num == 2:\n",
        "            output = self.create_CNN()\n",
        "        elif task_num == 3:\n",
        "            output = self.create_CNN()\n",
        "        else:\n",
        "            output = self.create_CNN_v2()\n",
        "        self.variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"input\")\n",
        "        self.variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"Conv_1\")\n",
        "        self.variables += tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"MaxPooling_1\")\n",
        "        self.variables += tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"Conv_2\")\n",
        "        self.variables += tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"MaxPooling_2\")\n",
        "        self.variables += tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"Dense_1\")\n",
        "        self.compile(output)\n",
        "        self.save_folder = save_folder\n",
        "        if os.path.exists(\"../Results/tensorboard/\" + save_folder + \"_train_\" + str(task_num)):\n",
        "            import shutil\n",
        "            shutil.rmtree(\"../Results/tensorboard/\" + save_folder + \"_train_\" + str(task_num))\n",
        "            shutil.rmtree(\"../Results/tensorboard/\" + save_folder + \"_test_\" + str(task_num))\n",
        "        os.makedirs(\"../Results/tensorboard/\" + save_folder + \"_train_\" + str(task_num), exist_ok=True)\n",
        "        os.makedirs(\"../Results/tensorboard/\" + save_folder + \"_test_\" + str(task_num), exist_ok=True)\n",
        "\n",
        "    def create_CNN(self):\n",
        "        max_pool = tf.reshape(self.x, shape=(-1, 28, 28, 1), name=\"input\")\n",
        "        for i in range(len(self.filters)):\n",
        "            convolution = self.convolution_layer(max_pool,\n",
        "                                                 filters=self.filters[i],\n",
        "                                                 filter_window=self.filter_windows[i],\n",
        "                                                 stride=1,\n",
        "                                                 n_input_channels=max_pool.get_shape().as_list()[3],\n",
        "                                                 name_scope=\"Conv_%d\" % (i + 1))\n",
        "            print(convolution.get_shape().as_list())\n",
        "            convolution = batch_normalization_from_scratch_conv(convolution, self.filters[i], self.phase_train)\n",
        "            # print(convolution.get_shape().as_list())\n",
        "            convolution = tf.nn.relu(convolution)\n",
        "            self.convolution_layers[\"Conv_%d\" % (i + 1)] = convolution\n",
        "            max_pool = max_pooling(convolution,\n",
        "                                   pooling_shape=self.pooling_shapes[i],\n",
        "                                   name_scope=\"MaxPooling_%d\" % (i + 1))\n",
        "        n_flatten_neurons = 1\n",
        "        for i in max_pool.get_shape().as_list()[1:]:\n",
        "            n_flatten_neurons *= i\n",
        "        flatten = tf.reshape(max_pool, [-1, n_flatten_neurons])\n",
        "\n",
        "        fully_connected_layer = self.dense(flatten, flatten.get_shape().as_list()[1], 256, name_scope=\"Dense_1\")\n",
        "        fully_connected_layer = batch_normalization_from_scratch_dense(fully_connected_layer, 256, self.phase_train)\n",
        "        fully_connected_layer = tf.nn.relu(fully_connected_layer)\n",
        "        fully_connected_layer = tf.nn.dropout(fully_connected_layer, 0.5)\n",
        "        output_layer = self.dense(fully_connected_layer, 256, 10, name_scope=\"Output_Layer\")\n",
        "        return output_layer\n",
        "\n",
        "    def create_CNN_v2(self):\n",
        "        max_pool = tf.reshape(self.x, shape=(-1, 28, 28, 1), name=\"input\")\n",
        "        for i in range(len(self.filters)):\n",
        "            convolution = self.convolution_layer(max_pool,\n",
        "                                                 filters=self.filters[i],\n",
        "                                                 filter_window=self.filter_windows[i],\n",
        "                                                 stride=1,\n",
        "                                                 n_input_channels=max_pool.get_shape().as_list()[3],\n",
        "                                                 name_scope=\"Conv_%d\" % (i + 1), trainable=False)\n",
        "            convolution = batch_normalization_from_scratch_conv(convolution, self.filters[i], self.phase_train)\n",
        "            convolution = tf.nn.relu(convolution)\n",
        "            self.convolution_layers[\"Conv_%d\" % (i + 1)] = convolution\n",
        "            max_pool = max_pooling(convolution,\n",
        "                                   pooling_shape=self.pooling_shapes[i],\n",
        "                                   name_scope=\"MaxPooling_%d\" % (i + 1))\n",
        "        n_flatten_neurons = 1\n",
        "        for i in max_pool.get_shape().as_list()[1:]:\n",
        "            n_flatten_neurons *= i\n",
        "        flatten = tf.reshape(max_pool, [-1, n_flatten_neurons])\n",
        "\n",
        "        fully_connected_layer = self.dense(flatten, flatten.get_shape().as_list()[1], 256, name_scope=\"Dense_1\", trainable=False)\n",
        "        fully_connected_layer = batch_normalization_from_scratch_dense(fully_connected_layer, 256, self.phase_train)\n",
        "        fully_connected_layer = tf.nn.relu(fully_connected_layer)\n",
        "        fully_connected_layer = tf.nn.dropout(fully_connected_layer, 0.5)\n",
        "        output_layer = self.dense(fully_connected_layer, 256, 2, name_scope=\"Output_Layer\")\n",
        "        return output_layer\n",
        "\n",
        "    def convolution_layer(self, x, filters=64, filter_window=(5, 5), stride=1, n_input_channels=1,\n",
        "                          name_scope=\"Conv2D_1\", trainable=True):\n",
        "        with tf.name_scope(name_scope):\n",
        "            weights = tf.Variable(\n",
        "                tf.truncated_normal(shape=[filter_window[0], filter_window[1], n_input_channels, filters], mean=0.0,\n",
        "                                    stddev=0.1),\n",
        "                name=\"Weights\", trainable=trainable)\n",
        "            self.convolution_weights[name_scope] = weights\n",
        "            biases = tf.Variable(tf.truncated_normal(shape=[filters], mean=0.0, stddev=0.1), name=\"Bias\",\n",
        "                                 trainable=trainable)\n",
        "            convolution = tf.nn.conv2d(x, weights, strides=[1, stride, stride, 1], padding=\"VALID\")\n",
        "\n",
        "            convolution += biases\n",
        "\n",
        "            output = tf.nn.relu(convolution)\n",
        "            # tf.summary.image(\"Weights\", weights)\n",
        "            # tf.summary.image(\"Biases\", biases)\n",
        "            return output\n",
        "\n",
        "    def compile(self, output):\n",
        "        with tf.name_scope(\"Cross_Entropy\"):\n",
        "            self.cross_entropy = tf.reduce_mean(\n",
        "                tf.nn.softmax_cross_entropy_with_logits(labels=self.y,\n",
        "                                                        logits=output))\n",
        "\n",
        "        with tf.name_scope(\"Train\"):\n",
        "            if self.task_num == 4:\n",
        "                self.optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-2).minimize(\n",
        "                    self.cross_entropy, var_list=tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"Output_Layer\"))\n",
        "            else:\n",
        "                self.optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-2).minimize(self.cross_entropy)\n",
        "        with tf.name_scope(\"accuracy\"):\n",
        "            correct_prediction = tf.equal(tf.argmax(output, axis=1),\n",
        "                                          tf.argmax(self.y, axis=1))\n",
        "            self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, dtype=tf.float32), name=\"train_accuracy\")\n",
        "\n",
        "        tf.summary.scalar(\"Cross_Entropy\", self.cross_entropy)\n",
        "        tf.summary.scalar(\"accuracy\", self.accuracy)\n",
        "\n",
        "    def dense(self, x, n_input_neurons, n_output_neurons, name_scope=\"Dense_1\", trainable=True):\n",
        "        with tf.name_scope(name_scope):\n",
        "            w = tf.Variable(tf.truncated_normal(shape=(n_input_neurons, n_output_neurons),\n",
        "                                                mean=0, stddev=0.1, seed=2018),\n",
        "                            name=\"Weight\", trainable=trainable)\n",
        "            b = tf.Variable(tf.constant(0.1, shape=[n_output_neurons]), name=\"Bias\", trainable=trainable)\n",
        "            hidden_output = tf.add(tf.matmul(x, w), b)\n",
        "            tf.summary.histogram(\"Weights\", w)\n",
        "\n",
        "            return hidden_output\n",
        "\n",
        "    def fit(self, n_epochs=50000, batch_size=64, verbose=1, keep_prob=0.5):\n",
        "        merge = tf.summary.merge_all()\n",
        "        init = tf.global_variables_initializer()\n",
        "        saver = tf.train.Saver()\n",
        "        with tf.Session() as sess:\n",
        "            train_file_writer = tf.summary.FileWriter(\n",
        "                logdir=\"../Results/tensorboard/%s\" % self.save_folder + \"_train_\" + str(self.task_num),\n",
        "                graph=sess.graph)\n",
        "            test_file_writer = tf.summary.FileWriter(\n",
        "                logdir=\"../Results/tensorboard/%s\" % self.save_folder + \"_test_\" + str(self.task_num),\n",
        "                graph=sess.graph)\n",
        "\n",
        "            sess.run(init)\n",
        "\n",
        "            sess.run(tf.global_variables_initializer())\n",
        "            mnist = load_data()\n",
        "            test_batch_xs, test_batch_ys = mnist.validation.next_batch(10000)\n",
        "            train_batch_xs, train_batch_ys = mnist.validation.next_batch(40000)\n",
        "            if self.task_num == 4:\n",
        "                indices = []\n",
        "                for idx, test_batch_y in enumerate(test_batch_ys):\n",
        "                    if test_batch_y[1] == 1 or test_batch_y[4] == 1:\n",
        "                        indices.append(idx)\n",
        "                test_batch_xs = test_batch_xs[indices]\n",
        "                test_batch_ys = test_batch_ys[indices]\n",
        "                test_batch_ys = test_batch_ys[:, [1, 4]]\n",
        "\n",
        "                indices = []\n",
        "                for idx, train_batch_y in enumerate(train_batch_ys):\n",
        "                    if train_batch_y[1] == 1 or train_batch_y[4] == 1:\n",
        "                        indices.append(idx)\n",
        "                train_batch_xs = train_batch_xs[indices]\n",
        "                train_batch_ys = train_batch_ys[indices]\n",
        "                train_batch_ys = train_batch_ys[:, [1, 4]]\n",
        "\n",
        "                for i in range(n_epochs):\n",
        "                    sess.run(self.optimizer,\n",
        "                             feed_dict={self.x: train_batch_xs, self.y: train_batch_ys, self.keep_prob: keep_prob,\n",
        "                                        self.phase_train: True})\n",
        "                    if (i + 1) % 50 == 0:\n",
        "                        summaries = sess.run(merge, feed_dict={self.x: train_batch_xs, self.y: train_batch_ys,\n",
        "                                                               self.keep_prob: keep_prob, self.phase_train: True})\n",
        "                        train_file_writer.add_summary(summaries, i)\n",
        "\n",
        "                        summaries = sess.run(merge, feed_dict={self.x: test_batch_xs, self.y: test_batch_ys,\n",
        "                                                               self.keep_prob: keep_prob, self.phase_train: False})\n",
        "                        test_file_writer.add_summary(summaries, i)\n",
        "\n",
        "                        train_accuracy, train_loss = sess.run((self.accuracy, self.cross_entropy),\n",
        "                                                              feed_dict={self.x: train_batch_xs, self.y: train_batch_ys,\n",
        "                                                                         self.keep_prob: keep_prob,\n",
        "                                                                         self.phase_train: True})\n",
        "\n",
        "                        test_accuracy, test_loss = sess.run([self.accuracy, self.cross_entropy],\n",
        "                                                            feed_dict={self.x: test_batch_xs, self.y: test_batch_ys,\n",
        "                                                                       self.keep_prob: keep_prob,\n",
        "                                                                       self.phase_train: False})\n",
        "                        if verbose == 1:\n",
        "                            print(\n",
        "                                \"Epoch: %5i\\t Train Accuracy: %.4f %%\\t Train Loss: %.4f\\t Validation Accuracy: %.4f %%\\t \"\n",
        "                                \"Validation \"\n",
        "                                \"Loss: %.4f\" % (\n",
        "                                    i + 1, 100.0 * train_accuracy, train_loss, 100.0 * test_accuracy, test_loss))\n",
        "                        # save_image(sess.run(self.convolution_weights[\"Conv_1\"]))\n",
        "                return\n",
        "            for i in range(n_epochs):\n",
        "                train_batch_xs, train_batch_ys = mnist.train.next_batch(batch_size)\n",
        "                sess.run(self.optimizer,\n",
        "                         feed_dict={self.x: train_batch_xs, self.y: train_batch_ys, self.keep_prob: keep_prob,\n",
        "                                    self.phase_train: True})\n",
        "                if (i + 1) % 50 == 0:\n",
        "                    summaries = sess.run(merge, feed_dict={self.x: train_batch_xs, self.y: train_batch_ys,\n",
        "                                                           self.keep_prob: keep_prob, self.phase_train: True})\n",
        "                    train_file_writer.add_summary(summaries, i)\n",
        "\n",
        "                    summaries = sess.run(merge, feed_dict={self.x: test_batch_xs, self.y: test_batch_ys,\n",
        "                                                           self.keep_prob: keep_prob, self.phase_train: False})\n",
        "                    test_file_writer.add_summary(summaries, i)\n",
        "\n",
        "                    train_accuracy, train_loss = sess.run((self.accuracy, self.cross_entropy),\n",
        "                                                          feed_dict={self.x: train_batch_xs, self.y: train_batch_ys,\n",
        "                                                                     self.keep_prob: keep_prob, self.phase_train: True})\n",
        "\n",
        "                    test_accuracy, test_loss = sess.run([self.accuracy, self.cross_entropy],\n",
        "                                                        feed_dict={self.x: test_batch_xs, self.y: test_batch_ys,\n",
        "                                                                   self.keep_prob: keep_prob, self.phase_train: False})\n",
        "                    if verbose == 1:\n",
        "                        print(\n",
        "                            \"Epoch: %5i\\t Train Accuracy: %.4f %%\\t Train Loss: %.4f\\t Validation Accuracy: %.4f %%\\t \"\n",
        "                            \"Validation \"\n",
        "                            \"Loss: %.4f\" % (\n",
        "                                i + 1, 100.0 * train_accuracy, train_loss, 100.0 * test_accuracy, test_loss))\n",
        "                    chkpt_path = \"../Results/CNN_iter_%4d.ckpt\" % (i + 1)\n",
        "                    saver.save(sess, chkpt_path)\n",
        "\n",
        "    def restore(self, path=\"../Results/CNN_iter_10000.ckpt\"):\n",
        "        with tf.Session() as sess:\n",
        "            if self.task_num == 2:\n",
        "                saver = tf.train.Saver()\n",
        "                saver.restore(sess, path)\n",
        "                save_image(sess.run(self.convolution_weights[\"Conv_1\"]))\n",
        "            if self.task_num == 3:\n",
        "                saver = tf.train.Saver()\n",
        "                saver.restore(sess, path)\n",
        "                self.save_a_5_image_conv_outputs(sess)\n",
        "            else:\n",
        "                saver = tf.train.Saver(self.variables)\n",
        "                saver.restore(sess, path)\n",
        "\n",
        "    def save_a_5_image_conv_outputs(self, sess):\n",
        "        mnist = load_data()\n",
        "        test_batch_xs, test_batch_ys = mnist.validation.next_batch(10000)\n",
        "        keep_prob = 0.5\n",
        "        for test_batch_x, test_batch_y in zip(test_batch_xs, test_batch_ys):\n",
        "            if test_batch_y[5] == 1:\n",
        "                test_batch_x = np.reshape(test_batch_x, newshape=(1, 784))\n",
        "                test_batch_y = np.reshape(test_batch_y, newshape=(1, 10))\n",
        "                conv_1_output, conv_2_output = sess.run(\n",
        "                    [self.convolution_layers[\"Conv_1\"], self.convolution_layers[\"Conv_2\"]],\n",
        "                    feed_dict={self.x: test_batch_x, self.y: test_batch_y,\n",
        "                               self.keep_prob: keep_prob,\n",
        "                               self.phase_train: False})\n",
        "                test_batch_x = np.reshape(test_batch_x, newshape=(28, 28))\n",
        "                save_image(test_batch_x, path=\"../Results/\", filename=\"5.png\")\n",
        "                save_image(conv_1_output, path=\"../Results/conv1_images/\", filename=\"Conv_1\")\n",
        "                save_image(conv_2_output, path=\"../Results/conv2_images/\", filename=\"Conv_2\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ecIx0-qZXk7w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_data():\n",
        "    from tensorflow.examples.tutorials.mnist import input_data\n",
        "    mnist = input_data.read_data_sets(\"mnist\", one_hot=True)\n",
        "    return mnist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X8Eg_UJ2XmHn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2513
        },
        "outputId": "293b8041-8c6e-4981-d384-a1c9bcd7025b"
      },
      "cell_type": "code",
      "source": [
        "filters = [64, 64]\n",
        "filter_windows = [(5, 5), (5, 5)]\n",
        "pooling_shapes = [(2, 2), (2, 2)]\n",
        "model = CNN(filters=filters, filter_windows=filter_windows, pooling_shapes=pooling_shapes, n_outputs=10, save_folder=\"CNN\", task_num=1)\n",
        "model.fit(n_epochs=10000, batch_size=64, verbose=1)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[None, 24, 24, 64]\n",
            "WARNING:tensorflow:From <ipython-input-4-f3fd060f9ccf>:19: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "keep_dims is deprecated, use keepdims instead\n",
            "[None, 8, 8, 64]\n",
            "WARNING:tensorflow:From <ipython-input-5-08ae7b4d6328>:123: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-6-a705ddb6a551>:3: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use urllib or similar directly.\n",
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting mnist/train-images-idx3-ubyte.gz\n",
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting mnist/train-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting mnist/t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting mnist/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "Epoch:    50\t Train Accuracy: 51.5625 %\t Train Loss: 1.3673\t Validation Accuracy: 58.9600 %\t Validation Loss: 1.2714\n",
            "Epoch:   100\t Train Accuracy: 71.8750 %\t Train Loss: 0.9356\t Validation Accuracy: 74.2900 %\t Validation Loss: 0.8568\n",
            "Epoch:   150\t Train Accuracy: 84.3750 %\t Train Loss: 0.5880\t Validation Accuracy: 80.5100 %\t Validation Loss: 0.6640\n",
            "Epoch:   200\t Train Accuracy: 89.0625 %\t Train Loss: 0.4586\t Validation Accuracy: 83.7100 %\t Validation Loss: 0.5586\n",
            "Epoch:   250\t Train Accuracy: 87.5000 %\t Train Loss: 0.4954\t Validation Accuracy: 86.2300 %\t Validation Loss: 0.4815\n",
            "Epoch:   300\t Train Accuracy: 90.6250 %\t Train Loss: 0.4126\t Validation Accuracy: 87.6700 %\t Validation Loss: 0.4348\n",
            "Epoch:   350\t Train Accuracy: 87.5000 %\t Train Loss: 0.3237\t Validation Accuracy: 89.1700 %\t Validation Loss: 0.3843\n",
            "Epoch:   400\t Train Accuracy: 84.3750 %\t Train Loss: 0.5231\t Validation Accuracy: 89.7300 %\t Validation Loss: 0.3701\n",
            "Epoch:   450\t Train Accuracy: 90.6250 %\t Train Loss: 0.3756\t Validation Accuracy: 90.7500 %\t Validation Loss: 0.3320\n",
            "Epoch:   500\t Train Accuracy: 85.9375 %\t Train Loss: 0.4659\t Validation Accuracy: 90.9200 %\t Validation Loss: 0.3299\n",
            "Epoch:   550\t Train Accuracy: 92.1875 %\t Train Loss: 0.2832\t Validation Accuracy: 91.1500 %\t Validation Loss: 0.3107\n",
            "Epoch:   600\t Train Accuracy: 95.3125 %\t Train Loss: 0.2769\t Validation Accuracy: 91.6900 %\t Validation Loss: 0.2940\n",
            "Epoch:   650\t Train Accuracy: 90.6250 %\t Train Loss: 0.3997\t Validation Accuracy: 92.5300 %\t Validation Loss: 0.2750\n",
            "Epoch:   700\t Train Accuracy: 92.1875 %\t Train Loss: 0.3046\t Validation Accuracy: 92.8900 %\t Validation Loss: 0.2610\n",
            "Epoch:   750\t Train Accuracy: 92.1875 %\t Train Loss: 0.2871\t Validation Accuracy: 92.3600 %\t Validation Loss: 0.2692\n",
            "Epoch:   800\t Train Accuracy: 90.6250 %\t Train Loss: 0.2387\t Validation Accuracy: 92.4400 %\t Validation Loss: 0.2631\n",
            "Epoch:   850\t Train Accuracy: 85.9375 %\t Train Loss: 0.3311\t Validation Accuracy: 93.0800 %\t Validation Loss: 0.2374\n",
            "Epoch:   900\t Train Accuracy: 95.3125 %\t Train Loss: 0.1843\t Validation Accuracy: 93.4600 %\t Validation Loss: 0.2441\n",
            "Epoch:   950\t Train Accuracy: 93.7500 %\t Train Loss: 0.2409\t Validation Accuracy: 93.3100 %\t Validation Loss: 0.2290\n",
            "Epoch:  1000\t Train Accuracy: 87.5000 %\t Train Loss: 0.3828\t Validation Accuracy: 93.2900 %\t Validation Loss: 0.2345\n",
            "Epoch:  1050\t Train Accuracy: 93.7500 %\t Train Loss: 0.2763\t Validation Accuracy: 94.0400 %\t Validation Loss: 0.2183\n",
            "Epoch:  1100\t Train Accuracy: 93.7500 %\t Train Loss: 0.2103\t Validation Accuracy: 94.0600 %\t Validation Loss: 0.2110\n",
            "Epoch:  1150\t Train Accuracy: 93.7500 %\t Train Loss: 0.2465\t Validation Accuracy: 94.1700 %\t Validation Loss: 0.2019\n",
            "Epoch:  1200\t Train Accuracy: 98.4375 %\t Train Loss: 0.1006\t Validation Accuracy: 94.2000 %\t Validation Loss: 0.2061\n",
            "Epoch:  1250\t Train Accuracy: 96.8750 %\t Train Loss: 0.1244\t Validation Accuracy: 94.5300 %\t Validation Loss: 0.1943\n",
            "Epoch:  1300\t Train Accuracy: 96.8750 %\t Train Loss: 0.1136\t Validation Accuracy: 94.0400 %\t Validation Loss: 0.2061\n",
            "Epoch:  1350\t Train Accuracy: 93.7500 %\t Train Loss: 0.2091\t Validation Accuracy: 94.5800 %\t Validation Loss: 0.1928\n",
            "Epoch:  1400\t Train Accuracy: 95.3125 %\t Train Loss: 0.1044\t Validation Accuracy: 94.8700 %\t Validation Loss: 0.1843\n",
            "Epoch:  1450\t Train Accuracy: 96.8750 %\t Train Loss: 0.1600\t Validation Accuracy: 94.9000 %\t Validation Loss: 0.1823\n",
            "Epoch:  1500\t Train Accuracy: 95.3125 %\t Train Loss: 0.1542\t Validation Accuracy: 94.9400 %\t Validation Loss: 0.1779\n",
            "Epoch:  1550\t Train Accuracy: 89.0625 %\t Train Loss: 0.2869\t Validation Accuracy: 94.8700 %\t Validation Loss: 0.1787\n",
            "Epoch:  1600\t Train Accuracy: 93.7500 %\t Train Loss: 0.1614\t Validation Accuracy: 95.1200 %\t Validation Loss: 0.1754\n",
            "Epoch:  1650\t Train Accuracy: 93.7500 %\t Train Loss: 0.1617\t Validation Accuracy: 94.9900 %\t Validation Loss: 0.1767\n",
            "Epoch:  1700\t Train Accuracy: 93.7500 %\t Train Loss: 0.1710\t Validation Accuracy: 95.2500 %\t Validation Loss: 0.1705\n",
            "Epoch:  1750\t Train Accuracy: 96.8750 %\t Train Loss: 0.1157\t Validation Accuracy: 95.2000 %\t Validation Loss: 0.1741\n",
            "Epoch:  1800\t Train Accuracy: 98.4375 %\t Train Loss: 0.0849\t Validation Accuracy: 95.5900 %\t Validation Loss: 0.1584\n",
            "Epoch:  1850\t Train Accuracy: 98.4375 %\t Train Loss: 0.0934\t Validation Accuracy: 95.6300 %\t Validation Loss: 0.1634\n",
            "Epoch:  1900\t Train Accuracy: 93.7500 %\t Train Loss: 0.1640\t Validation Accuracy: 94.7300 %\t Validation Loss: 0.1820\n",
            "Epoch:  1950\t Train Accuracy: 93.7500 %\t Train Loss: 0.1967\t Validation Accuracy: 95.2600 %\t Validation Loss: 0.1678\n",
            "Epoch:  2000\t Train Accuracy: 90.6250 %\t Train Loss: 0.4324\t Validation Accuracy: 95.4500 %\t Validation Loss: 0.1618\n",
            "Epoch:  2050\t Train Accuracy: 90.6250 %\t Train Loss: 0.2360\t Validation Accuracy: 95.2200 %\t Validation Loss: 0.1586\n",
            "Epoch:  2100\t Train Accuracy: 96.8750 %\t Train Loss: 0.1174\t Validation Accuracy: 95.2000 %\t Validation Loss: 0.1647\n",
            "Epoch:  2150\t Train Accuracy: 93.7500 %\t Train Loss: 0.1694\t Validation Accuracy: 95.5200 %\t Validation Loss: 0.1539\n",
            "Epoch:  2200\t Train Accuracy: 100.0000 %\t Train Loss: 0.0611\t Validation Accuracy: 95.7600 %\t Validation Loss: 0.1475\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-82e00fdb6919>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpooling_shapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_windows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilter_windows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpooling_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpooling_shapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_folder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"CNN\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-08ae7b4d6328>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, n_epochs, batch_size, verbose, keep_prob)\u001b[0m\n\u001b[1;32m    217\u001b[0m                 sess.run(self.optimizer,\n\u001b[1;32m    218\u001b[0m                          feed_dict={self.x: train_batch_xs, self.y: train_batch_ys, self.keep_prob: keep_prob,\n\u001b[0;32m--> 219\u001b[0;31m                                     self.phase_train: True})\n\u001b[0m\u001b[1;32m    220\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m50\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                     summaries = sess.run(merge, feed_dict={self.x: train_batch_xs, self.y: train_batch_ys,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "NL-flL9nYHV5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}