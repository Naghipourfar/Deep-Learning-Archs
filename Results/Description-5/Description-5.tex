\documentclass{article}
\pagestyle{empty}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{subfigure}
\usepackage[margin=2.5cm]{geometry}
\setlength\parindent{0pt}
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{float}
\usepackage[extrafootnotefeatures]{xepersian}
\settextfont[Scale=1]{B Zar}

\begin{document}
\begin{titlepage}
	\centering
	{\scshape\LARGE به نام خداوند بخشنده و مهربان \par}
	\vspace{2cm}
	{\huge\bfseries یادگیری عمیق\par}
	\vspace{3cm}
	{\Large\bfseries  تمرین پنجم \par}
	\vspace{3cm}
	{\Large\itshape 		محسن نقی‌پورفر		94106757\par}
	\vspace{0.25cm}
	\vfill
	\end{titlepage}

\section{\lr{Regularization}}
\subsection{\lr{BatchNormalization}}
\subsubsection{تاثیر اضافه کردن \lr{BatchNormalization}}
\subsubsection{تعداد پارامتر های افزوده شده}
\subsubsection{پیاده سازی تابع این لایه}
\subsection{\lr{Dropout}}
\subsubsection{تاثیر اضافه کردن \lr{Dropout}}
\subsubsection{فرق در آموزش و تست}
\subsubsection{پیاده سازی تابع این لایه}
\section{\lr{Google Colab}}
\subsection{گزارش نتیجه و مراحل اجرا در این محیط}
\subsection{مقایسه در حالت وجود یا عدم وجود منظم ساز‌ها}
\subsection{گزارش نتیجه در اثر وجود دو منظم ساز}

\section{\lr{Visualization}}
\subsection{توضیح در مورد شبکه \lr{VGG}}
این شبکه در سال ۲۰۱۴ در کنفرانس \lr{ICLR} معرفی شد. این شبکه دو نوع \lr{VGG16} و \lr{VGG19} دارد که به ترتیب از ۱۶ و ۱۹ لایه تشکیل شده‌اند. در این شبکه‌ها فیلتر های وزن در لایه‌های کانوولوشنی بسیار کوچک می‌باشند و در سایز ۳ در ۳ می‌باشند که طبق گفته مقاله این شبکه، این فیلتر‌ها باعث عمیق‌تر کردن شبکه و عین حال نتیجه بهتر نسبت به مدل‌های مشابه گرفتن‌، می‌باشند. در واقع وجود این فیلتر‌ها باعث شده تا تعداد لایه ‌های شبکه تا ۱۶ یا ۱۹ لایه پیش‌برود و دقت آن نیز نسبت به مدل‌های مشابه بهتر باشد. این شبکه  برنده مسابقه \lr{IMAGENET Challenge} در تسک \lr{Localization} در سال ۲۰۱۴ و برنده مقام دومی در تسک \lr{Classification} در همان سال می‌باشد. این مسابقه که هرساله برگزار می‌شود دارای دیتای بسیار معروفی به نام \lr{ILSVRC} باشد. این شبکه با بیشتر کردن عمق‌ خود با استفاده از فیکس کردن سایز فیلتر‌های وزن لایه‌های کانوولوشن و بسیار کوچک بودن سایز آن سعی در بهتر کردن دقت خود داشته است و در این زمینه نیز موفق بوده است. طبق گفته نویسندگان این مقاله، این شبکه نه تنها برای دیتای مسابقه \lr{ILSVRC} بسیار خوب عمل می‌کند بلکه روی دیتای مسابقه‌های دیگر نیز بسیار خود عمل کرده است. این شبکه علاوه بر دیتای \lr{ILSVRC} در مقاله‌ خود بر روی دیتاهای \lr{VOC-2012}،\lr{VOC-2007}، \lr{Caltech-101} و \lr{Caltech-256} نیز برای تسک های \lr{Classification} و \lr{Localization} تست شده است.  نتایج مربوط به دقت این شبکه در جداول زیر آمده است. این شبکه برای ارزیابی روی دیتای \lr{ILSVRC} از دو معیار \lr{Top-1 Error} و \lr{Top-5 Error} استفاده کرده است که اولی نسبت تعداد عکس های به اشتباه طبقه‌بندی شده در داده تست است ولی دومی نسبت تعداد عکس‌هایی به کل است که کلاس درست برای این عکس‌ها در ۵ کلاس اول محتمل که شبکه پیش‌بینی کرده است می‌باشد. بنابراین مشخص است که خطای \lr{Top-5} نسبت به خطای \lr{Top-1} همیشه مقدار کمتری برای شبکه‌های مختلف خواهد داشت. علت وجود این نوع جدید از خطا نیز وجود ۱۰۰۰ کلاس در دیتای \lr{ILSVRC} می‌باشد که تعداد خیلی زیادی است و شهود آن به این معنی است که اگر شبکه برای عکس ورودی از بین ۱۰۰۰ کلاس، کلاس درست را در ۵ کلاس محتمل‌ترین برای یک عکس ببیند، قابل قبول است و خطا نیست. نتایج مربوط به این خطا نیز برای این دیتا در جدول زیر آمده است.
‌\begin{figure}[H]
	\centerline{\includegraphics[width=13cm, height=5cm]{VGG_results_1}}
	\caption{جدول مربوط نتایج شبکه \lr{VGG} روی دیتای \lr{ILSVRC}}
\end{figure}

‌\begin{figure}[H]
	\centerline{\includegraphics[width=13cm, height=5cm]{VGG_results_2}}
	\caption{جدول مربوط نتایج شبکه \lr{VGG} روی دیتاهای دیگر}
\end{figure}
\subsection{توضیح معماری شبکه \lr{VGG}}
این شبکه از ۱۶ لایه تشکیل شده است. عکس مربوط به معماری شبکه و پارامتر‌های هر لایه و ابعاد هر لایه در عکس دوم قابل مشاهده است. این شبکه در کل حدود ۱۳۸ میلیون 
پارامتر قابل یادگیری دارد. همچنین نوع لایه‌ها در عکس دوم و اول قابل مشاهده است.
\begin{figure}[H]
	\centerline{\includegraphics[width=13cm, height=5cm]{VGG16.png}}
	\caption{معماری شبکه \lr{VGG16}}
\end{figure}
\begin{figure}[H]
	\centering
	\subfigure{\includegraphics[width=5cm, height=15cm]{VGG16.pdf}}
	\subfigure{\includegraphics[width=5cm, height=15cm]{VGG16_summary}}
	\caption {ابعاد وزن‌ها و ورودی و تعداد پارامتر‌های قابل یادگیری در لایه‌های مختلف در شبکه \lr{VGG16}}
\end{figure}
\subsection{گزارش فیلتر‌های هر لایه و مقایسه اولین و آخرین فیلتر}
\subsection{تحلیل نتایج لایه‌های ۳ و ۱۳ حاصل از ورودی های جدید شبکه}
\section{\lr{DeConvolution}}
\subsection{رسم شبکه عصبی و مشخصات هر لایه}
همانطور که از کد مشخص است، این شبکه، در قسمت \lr{encoder} همان شبکه \lr{VGG16} می‌باشد که معماری و مشخصات آن در سوال دوم کشیده و توضیح داده شد. این شبکه در قسمت \lr{Decoder} خود، دقیقا برعکس \lr{VGG} عمل می‌کند یعنی به ازای هر لایه \lr{Convolution} یک لایه \lr{Deconvolution} و به ازای هر لایه \lr{Pooling} یک لایه \lr{Unpooling} گذاشته شده است و در نهایت در خروجی شبکه، سایز تصویر برابر سایز ورودی شبکه می‌باشد.
\subsection{کاربر در شبکه های عمیق}
\subsection{نحوه عملکر این لایه و تفاوت با لایه \lr{Convolution}}


\end{document}