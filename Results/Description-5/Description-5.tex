\documentclass{article}
\pagestyle{empty}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{subfigure}
\usepackage[margin=2.5cm]{geometry}
\setlength\parindent{0pt}
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{float}
\usepackage[extrafootnotefeatures]{xepersian}
\renewcommand{\labelitemi}{$\bullet$}
\settextfont[Scale=1]{B Zar}

\begin{document}
\begin{titlepage}
	\centering
	{\scshape\LARGE به نام خداوند بخشنده و مهربان \par}
	\vspace{2cm}
	{\huge\bfseries یادگیری عمیق\par}
	\vspace{3cm}
	{\Large\bfseries  تمرین پنجم \par}
	\vspace{3cm}
	{\Large\itshape 		محسن نقی‌پورفر		94106757\par}
	\vspace{0.25cm}
	\vfill
	\end{titlepage}

\section{\lr{Regularization}}
\subsection{\lr{BatchNormalization}}
\subsubsection{تاثیر اضافه کردن \lr{BatchNormalization}}
در صورت اضافه کردن لایه \lr{BatchNormalization} تاثیرات زیر به شبکه اعمال می‌شود:
\begin{itemize}
	\item سرعت آموزش شبکه با توجه به حل شدن مسئله \lr{Internal Covariate Shift} زیاد می‌شود.
	\item مسئله وابستگی ورودی‌های هرلایه و لایه بعدی حل می‌شود و این باعث ایجاد امکان یادگیری و بهینه‌سازی شبکه با استفاده از \lr{learning rate} ‌های بالا می‌شود.
	\item با حل مسئله \lr{ICS}،‌می‌توان از شبکه‌‌های عمیق‌تر نیز برای مسئله خاص استفاده کرد و به نتایج معمولا بهتری نسبت به شبکه با عمق کمتر دست یافت.
	\item این لایه، باعث نرمالیزه کردن توزیع خروجی هر لایه در هر نورون می‌شود و این نیز باعث سرعت بخشیدن به فرآیند یادگیری می‌شود.
	\item با استفاده از آماره‌های \lr{Batch}، باعث می‌شود که هر شبکه منظم شود و \lr{Regularize} شود و این منظم شدن را نسبت به \lr{Overfitting} می‌گوییم. پس این لایه باعث منظم سازی شبکه و در نتیجه جلوگیری از \lr{Overfitting} می‌شود.
	\item افزودن این لایه به شبکه اجازه مقداردهی اولیه ضعیف(\lr{Poor Initialization}) به وزن‌ها و پارامتر‌های یادگیری شبکه را می‌دهد بدون آنکه در فرآیند یادگیری مشکلی ایجاد شود.
	\item مسئله \lr{explode}شدن و یا \lr{vanish}شدن گرادیان را به حد قابل قبولی حل می‌کند.
	\item این لایه با توجه به محدود کردن ورودی تابع فعال‌ساز در ناحیه خاصی که برای تابع فعال ساز \lr{relevant}می‌باشد، تا حدی از اشباع شدن خروجی تابع جلوگیری می‌کند و به نوعی شبکه را \lr{stablize} می‌کند.
	\item اختلافات خطی بین \lr{Batch}‌های مختلف را از بین می‌برد.
	\item ارزش و توانایی مدل را با اضافه کردن دو پارامتر قابل یادگیری \lr{$\gamma$} و \lr{$\beta$} افزایش می‌دهد.
	\item در زمان تست با توجه به اینکه ممکن است \lr{Batch} برای داده‌های تست خیلی کوچک باشد، با استفاده \lr{EMA} تخمین خوبی از میانگین و واریانس \lr{Batch} می‌زند.
\end{itemize}
\subsubsection{تعداد پارامتر های افزوده شده}
در اثر اضافه کردن لایه \lr{BatchNormalization} به هر لایه، به ازای هر نورون آن لایه، دو پارامتر \lr{$\gamma$}و \lr{$\beta$} اضافه می‌شود که قابل یادگیری هستند.
\subsubsection{پیاده سازی تابع این لایه}
عکس‌های زیر، پیاده سازی لایه \lr{BatchNormalization} می‌باشند. همانطور که از اسم آنها نیز پیداست، عکس اول مربوط به پیاده‌سازی این لایه برای بعد از لایه \lr{Convolution} است و عکس دوم مربوط به پیاده‌سازی این لایه بعد از یک لایه \lr{Fully Connected} می‌باشد.
\subsection{\lr{Dropout}}
\subsubsection{تاثیر اضافه کردن \lr{Dropout}}
اضافه کردن این منظم‌ساز به هر لایه، تاثیرات زیر را همراه خود می‌آورد:
\begin{itemize}
	\item تاثیر مشخص اول، همان منظم‌ساز بودن آن است، به این معنی که از \lr{overfitting} جلوگیری می‌کند.
	\item باتوجه به منظم‌ساز بودن آن، به \lr{feature co-adaptation} کمک می‌شود.
	\item باتوجه به اینکه منظم ‌ساز می‌باشد، به فرآیند یادگیری، نویز اضافه می‌کند که این یکی از تاثیرات اضافه کردن منظم‌ساز است.
	\item باعث بیشتر شدن \lr{Sparsity} در لایه‌های پنهان شبکه‌ می‌شود که این نیز از تاثیرات اضافه کردن این منظم‌ساز است.
	\item در واقع شبیه میانگین گیری از تمام \lr{$2^H$} حالت ممکن برای مدل‌ها می‌باشد(در صورتی که به یک لایه پنهان با \lr{H} نورون اعمال شود.). در این حالت وزن‌ها به نوعی \lr{Shared} هستند و این باعث منظم شدن مدل شده و از \lr{overfitting} جلوگیری می‌کند.
\end{itemize} 
\subsubsection{فرق در آموزش و تست}
در هنگام آموزش، ما از یک توزیع \lr{Binomial} با احتمال موفقیت \lr{P}استفاده می‌کنیم و نورون‌های لایه‌ای که این منظم‌ساز روی آن اعمال شده است را به طور کامل حذف می‌کنیم. در واقع هر نورون با احتمال \lr{P} در شبکه حضور دارد و یک سری از نورون‌ها در فرآیند یادگیری حذف می‌شوند. اما در فرآیند تست، همه نورون‌ها در شبکه به طور قطع حاضر هستند، اما وزن \lr{Connection}آنها در عدد \lr{P} ضرب می‌شود و به نوعی \lr{Expected} گیری انجام می‌دهیم برای خروجی هر نورون و تاثیرات آن روی نورون‌های لایه بعدی. یعنی، انتظار داریم که خروجی نورون که با احتمال \lr{P} از شبکه‌ در فرآیند یادگیری حذف می‌شد،‌خروجی آن به ازای وزن‌های \lr{Pw} باشد. شکل زیر که از مقاله خود جناب آقای \lr{Hinton} از مقاله \lr{Dropout} آمده است، فرق بین اعمال این منظم ساز در فرآیند تست و آموزش را به خوبی توضیح می‌دهد.
\subsubsection{پیاده سازی تابع این لایه}
در عکس زیر پیاده‌سازی مربوط به این منظم‌ساز روی ورودی خود آمده است.
\section{\lr{Google Colab}}
\subsection{گزارش نتیجه و مراحل اجرا در این محیط}
\subsection{مقایسه در حالت وجود یا عدم وجود منظم ساز‌ها}
\subsection{گزارش نتیجه در اثر وجود دو منظم ساز}

\section{\lr{Visualization}}
\subsection{توضیح در مورد شبکه \lr{VGG}}
این شبکه در سال ۲۰۱۴ در کنفرانس \lr{ICLR} معرفی شد. این شبکه دو نوع \lr{VGG16} و \lr{VGG19} دارد که به ترتیب از ۱۶ و ۱۹ لایه تشکیل شده‌اند. در این شبکه‌ها فیلتر های وزن در لایه‌های کانوولوشنی بسیار کوچک می‌باشند و در سایز ۳ در ۳ می‌باشند که طبق گفته مقاله این شبکه، این فیلتر‌ها باعث عمیق‌تر کردن شبکه و عین حال نتیجه بهتر نسبت به مدل‌های مشابه گرفتن‌، می‌باشند. در واقع وجود این فیلتر‌ها باعث شده تا تعداد لایه ‌های شبکه تا ۱۶ یا ۱۹ لایه پیش‌برود و دقت آن نیز نسبت به مدل‌های مشابه بهتر باشد. این شبکه  برنده مسابقه \lr{IMAGENET Challenge} در تسک \lr{Localization} در سال ۲۰۱۴ و برنده مقام دومی در تسک \lr{Classification} در همان سال می‌باشد. این مسابقه که هرساله برگزار می‌شود دارای دیتای بسیار معروفی به نام \lr{ILSVRC} باشد. این شبکه با بیشتر کردن عمق‌ خود با استفاده از فیکس کردن سایز فیلتر‌های وزن لایه‌های کانوولوشن و بسیار کوچک بودن سایز آن سعی در بهتر کردن دقت خود داشته است و در این زمینه نیز موفق بوده است. طبق گفته نویسندگان این مقاله، این شبکه نه تنها برای دیتای مسابقه \lr{ILSVRC} بسیار خوب عمل می‌کند بلکه روی دیتای مسابقه‌های دیگر نیز بسیار خود عمل کرده است. این شبکه علاوه بر دیتای \lr{ILSVRC} در مقاله‌ خود بر روی دیتاهای \lr{VOC-2012}،\lr{VOC-2007}، \lr{Caltech-101} و \lr{Caltech-256} نیز برای تسک های \lr{Classification} و \lr{Localization} تست شده است.  نتایج مربوط به دقت این شبکه در جداول زیر آمده است. این شبکه برای ارزیابی روی دیتای \lr{ILSVRC} از دو معیار \lr{Top-1 Error} و \lr{Top-5 Error} استفاده کرده است که اولی نسبت تعداد عکس های به اشتباه طبقه‌بندی شده در داده تست است ولی دومی نسبت تعداد عکس‌هایی به کل است که کلاس درست برای این عکس‌ها در ۵ کلاس اول محتمل که شبکه پیش‌بینی کرده است می‌باشد. بنابراین مشخص است که خطای \lr{Top-5} نسبت به خطای \lr{Top-1} همیشه مقدار کمتری برای شبکه‌های مختلف خواهد داشت. علت وجود این نوع جدید از خطا نیز وجود ۱۰۰۰ کلاس در دیتای \lr{ILSVRC} می‌باشد که تعداد خیلی زیادی است و شهود آن به این معنی است که اگر شبکه برای عکس ورودی از بین ۱۰۰۰ کلاس، کلاس درست را در ۵ کلاس محتمل‌ترین برای یک عکس ببیند، قابل قبول است و خطا نیست. نتایج مربوط به این خطا نیز برای این دیتا در جدول زیر آمده است.
‌\begin{figure}[H]
	\centerline{\includegraphics[width=13cm, height=5cm]{VGG_results_1}}
	\caption{جدول مربوط نتایج شبکه \lr{VGG} روی دیتای \lr{ILSVRC}}
\end{figure}

‌\begin{figure}[H]
	\centerline{\includegraphics[width=13cm, height=5cm]{VGG_results_2}}
	\caption{جدول مربوط نتایج شبکه \lr{VGG} روی دیتاهای دیگر}
\end{figure}
\subsection{توضیح معماری شبکه \lr{VGG}}
این شبکه از ۱۶ لایه تشکیل شده است. عکس مربوط به معماری شبکه و پارامتر‌های هر لایه و ابعاد هر لایه در عکس دوم قابل مشاهده است. این شبکه در کل حدود ۱۳۸ میلیون 
پارامتر قابل یادگیری دارد. همچنین نوع لایه‌ها در عکس دوم و اول قابل مشاهده است.
\begin{figure}[H]
	\centerline{\includegraphics[width=13cm, height=5cm]{VGG16.png}}
	\caption{معماری شبکه \lr{VGG16}}
\end{figure}
\begin{figure}[H]
	\centering
	\subfigure{\includegraphics[width=5cm, height=15cm]{VGG16.pdf}}
	\subfigure{\includegraphics[width=5cm, height=15cm]{VGG16_summary}}
	\caption {ابعاد وزن‌ها و ورودی و تعداد پارامتر‌های قابل یادگیری در لایه‌های مختلف در شبکه \lr{VGG16}}
\end{figure}
\subsection{گزارش فیلتر‌های هر لایه و مقایسه اولین و آخرین فیلتر}
\subsection{تحلیل نتایج لایه‌های ۳ و ۱۳ حاصل از ورودی های جدید شبکه}
\section{\lr{DeConvolution}}
\subsection{رسم شبکه عصبی و مشخصات هر لایه}
همانطور که از کد مشخص است، این شبکه، در قسمت \lr{encoder} همان شبکه \lr{VGG16} می‌باشد که معماری و مشخصات آن در سوال دوم کشیده و توضیح داده شد. این شبکه در قسمت \lr{Decoder} خود، دقیقا برعکس \lr{VGG} عمل می‌کند یعنی به ازای هر لایه \lr{Convolution} یک لایه \lr{Deconvolution} و به ازای هر لایه \lr{Pooling} یک لایه \lr{Unpooling} گذاشته شده است و در نهایت در خروجی شبکه، سایز تصویر برابر 
سایز ورودی شبکه می‌باشد.
\begin{figure}[H]
	\centerline{\includegraphics[width=10cm, height=13cm]{DeConvnet}}
	\caption{عکس مربوط به رسم شبکه‌عصبی موجود در کد \lr{net.py} که همان شبکه معروف \lr{Deconvnet} است.}
\end{figure}

\begin{figure}[H]
	\centerline{\includegraphics[width=13cm, height=5cm]{DeConvnet-2}}
	\caption{عکس مربوط به شبکه \lr{Deconvnet} در اینترنت}
\end{figure}
\subsection{کاربر در شبکه های عمیق}
\subsection{نحوه عملکر این لایه و تفاوت با لایه \lr{Convolution}}


\end{document}